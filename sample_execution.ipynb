{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Train Data\"\"\"\n",
    "#code for reading data\n",
    "imgdir = []   #shall contain the full path of the image\n",
    "ytr = []         #shall contain label of each image\n",
    "#xtr = np.matrix([[]])         #shall contain each image\n",
    "\n",
    "#xtr = np.zeros((257,262144))   #modified\n",
    "xtr = np.zeros((1300,10000))\n",
    "\n",
    "datatype = ['train', 'valid']\n",
    "studylabel = {'negative':0, 'positive':1}\n",
    "\n",
    "BASE_DIR = 'mura_humerus'\n",
    "\n",
    "traindir = BASE_DIR + '/' + datatype[0]  #shows training directory\n",
    "\n",
    "patients = os.listdir(traindir)  #all the patient ID's are now into this\n",
    "\n",
    "i = 0\n",
    "for patient in patients:\n",
    "    directory = traindir + '/' + patient    #patient directory train/patientxxxx/\n",
    "    studies = os.listdir(directory)         #creates list of studies in each patient directory\n",
    "    for study in studies:\n",
    "        images = os.listdir(directory + '/' + study)\n",
    "        for image in images:\n",
    "            imgdir = np.append(imgdir, directory + '/' + study + '/' + image)    #add image addresses in array\n",
    "            ytr = np.append(ytr, studylabel[study.split('_')[1]])        #keep adding labels of each images\n",
    "            file = mpimg.imread(imgdir[i])\n",
    "            if (len(np.shape(file))>2):\n",
    "                pixels = np.matrix(file[:,:,0])     #takes matrix format of one image\n",
    "                pixels = np.reshape(pixels, (1,len(pixels)*len(pixels.T)))   #flattens the image to 1-D\n",
    "            else:\n",
    "                pixels = np.matrix(file)\n",
    "                pixels = np.reshape(pixels, (1,len(pixels)*len(pixels.T)))\n",
    "                \n",
    "            if (np.shape(pixels)[1] < 10000):    #modified from 262144\n",
    "                zeroarray = np.matrix(np.zeros((1,(10000-np.shape(pixels)[1]))))\n",
    "                pixels = np.concatenate((pixels,zeroarray), axis=1)\n",
    "                #print(pixels)\n",
    "                #xtr = np.append(xtr, np.array(file[:,:,0]), axis=0)\n",
    "                ###xtr = np.concatenate(xtr,pixels, axis=1)               #load and add images in \n",
    "                xtr[i] = pixels               #load and add images in \n",
    "                #print ('i from if=', i)\n",
    "            else:\n",
    "                xtr[i] = np.matrix(pixels[:,0:10000])     #modified from 262144\n",
    "                #print(pixels)\n",
    "                #print ('i from else=', i)\n",
    "                \n",
    "            i=i+1\n",
    "            \n",
    "xtrain = xtr[0:i,:]\n",
    "ytrain = ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Normalize data\"\"\"\n",
    "for i in range (len(xtrain)):\n",
    "    if np.amax(xtrain[i]) != 0:\n",
    "        xtrain[i] = xtrain[i] / np.amax(xtrain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Eliminating all zero data\"\"\"\n",
    "xtr = np.zeros((np.shape(xtrain)))\n",
    "ytr = []\n",
    "j = 0\n",
    "i = 0\n",
    "while i < (len(ytrain)):\n",
    "    if np.amax(xtrain[i]) != 0:\n",
    "        xtr[j] = xtrain[i]\n",
    "        ytr.append(ytrain[i])\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "    \n",
    "xtr = xtr[0:j,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1272, 10000)\n",
      "(1260, 10000)\n",
      "(1272,)\n",
      "(1260,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verification\"\"\"\n",
    "print(np.shape(xtrain))\n",
    "print(np.shape(xtr))\n",
    "print(np.shape(ytrain))\n",
    "print(np.shape(ytr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for calculating error\"\"\"         #approved\n",
    "def errcalc(y,ypred):\n",
    "    if len(y) != len(ypred):\n",
    "        print('The inputs must be same size')\n",
    "        return -1\n",
    "    else:\n",
    "        error = np.sum(np.absolute(np.subtract(y,ypred)))\n",
    "    return error/(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for creating a 5 layer NN. Creates, Compiles and Returns the Model\"\"\"   #approved\n",
    "def make_nn(inlayersize, h1size, h2size, h3size, h4size, h5size, h6size, h7size, h8size, h9size, h10size, outlayersize):\n",
    "    #Training data in 'xtr'\n",
    "    #Labels in 'ytr'\n",
    "\n",
    "    #input layer\n",
    "    inp_layer = Input(shape=(inlayersize,))\n",
    "\n",
    "\n",
    "    #hidden layer 1\n",
    "    H1 = Dense(h1size, activation = 'relu')(inp_layer)\n",
    "\n",
    "    #hidden layer 2\n",
    "    H2 = Dense(h2size, activation='relu')(H1)\n",
    "\n",
    "    #hidden layer 3\n",
    "    H3 = Dense(h3size, activation = 'relu')(H2)\n",
    "\n",
    "    #hidden layer 4\n",
    "    H4 = Dense(h4size, activation = 'relu')(H3)\n",
    "    \n",
    "    #hidden layer 5\n",
    "    H5 = Dense(h5size, activation = 'relu')(H4)\n",
    "\n",
    "    #hidden layer 6\n",
    "    H6 = Dense(h6size, activation = 'relu')(H5)\n",
    "\n",
    "    #hidden layer 7\n",
    "    H7 = Dense(h7size, activation = 'relu')(H6)\n",
    "    \n",
    "    #hidden layer 8\n",
    "    H8 = Dense(h8size, activation = 'relu')(H7)\n",
    "    \n",
    "    #hidden layer 9\n",
    "    H9 = Dense(h9size, activation = 'relu')(H8)\n",
    "\n",
    "    #hidden layer 10\n",
    "    H10 = Dense(h10size, activation = 'relu')(H9)\n",
    "\n",
    "    #output layer\n",
    "    out_layer = Dense(outlayersize, activation='sigmoid')(H10)\n",
    "\n",
    "\n",
    "\n",
    "    ###creating the neural network\n",
    "    shallow_nn = Model(inp_layer, out_layer)\n",
    "\n",
    "\n",
    "    ###Compiling model\n",
    "    shallow_nn.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "    ### Returning compiled model\n",
    "    \n",
    "    return shallow_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for fitting into a model\"\"\"        #approved\n",
    "def train_nn(shallow_nn,xtr,ytr, batchsize, epchs, shffle):\n",
    "    shallow_nn.fit(xtr, ytr, batch_size=batchsize, epochs=epchs, shuffle=shffle)\n",
    "    return shallow_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for Predicting, and calculating error\"\"\"   #approved\n",
    "def predict_nn(shallow_nn, x, y):\n",
    "    pred = shallow_nn.predict(x)\n",
    "    y_pred = np.round(pred)\n",
    "    y_pred = np.ravel(y_pred)\n",
    "    valid_err = errcalc(y,y_pred)\n",
    "    return y_pred, valid_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for calculating Kappa\"\"\"\n",
    "def kappa_calc(y, y_pred):\n",
    "    \n",
    "    if len(y) != len(y_pred):\n",
    "        print('Input array sizes must match')\n",
    "        return -1\n",
    "    else:    \n",
    "        #parameters\n",
    "        total_data = len(y)\n",
    "        agree_num = len(np.where(y == y_pred)[0])    #where model and specialists agree\n",
    "\n",
    "        #specialists saying yes and no\n",
    "        spec_yes = len(np.where(np.array(y) == 1)[0])\n",
    "        spec_no = total_data - spec_yes\n",
    "\n",
    "        #model saying yes\n",
    "        model_yes = len(np.where(np.array(y_pred) == 1)[0])\n",
    "        model_no = total_data - model_yes\n",
    "\n",
    "        #calculating p_o\n",
    "        p_o = agree_num / total_data\n",
    "\n",
    "        #calculate p_e [raters randomly agreeing]\n",
    "        rand_yes = (spec_yes/ total_data) * (model_yes/ total_data)\n",
    "        rand_no = (spec_no / total_data) * (model_no / total_data)\n",
    "        p_e = rand_yes + rand_no\n",
    "\n",
    "\n",
    "        #kappa\n",
    "        kappa = (p_o - p_e) / (1 - p_e)\n",
    "        #kappa = 0\n",
    "        print('kappa value is = ', kappa)\n",
    "        print('po value is', p_o)\n",
    "        print('agree num', agree_num)\n",
    "        print('pe value is', p_e)\n",
    "        print('rand_yes ', rand_yes)\n",
    "        print('rand_no ', rand_no)\n",
    "        print('spec_yes ' , spec_yes)\n",
    "        print('spec_no', spec_no)\n",
    "        print('model_yes ', model_yes)\n",
    "        print('model_no ', model_no)\n",
    "        return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for calculating weighted error Epsilon\"\"\" #approved\n",
    "def weighted_error(y,ypred,d):\n",
    "    t = len(d)\n",
    "    vec = np.abs(np.subtract(y,ypred))\n",
    "    sumerror = 0.0\n",
    "    for i in range (t):\n",
    "        sumerror = sumerror + (d[i]*vec[i])\n",
    "    return sumerror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distr_upd(D, epsilon, alpha, y, h_x):       #approved\n",
    "    z = 2.0* math.sqrt(epsilon*(1-epsilon))\n",
    "    D_out = np.zeros(len(D))\n",
    "    ##function to find mismatch vector\n",
    "    ylabel = np.array(y)\n",
    "    hypo = np.array(h_x)\n",
    "    mismatch_vec = np.where(ylabel!=hypo, -1, 1)\n",
    "    for i in range (len(D)):\n",
    "        D_out[i] = (D[i]/z) * math.exp( (-alpha)* mismatch_vec[i])\n",
    "    D_out = D_out / np.sum(D_out)\n",
    "    return D_out/np.sum(D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for Predicting Ensambled output\"\"\"   #approved\n",
    "def ens_pred_nn(shallow_nn, x, y, alpha):\n",
    "        nbsample, __ = np.shape(x)\n",
    "        y_pred = np.zeros((nbsample))\n",
    "        for nn, alph in zip(shallow_nn, alpha):\n",
    "            y_temp, __ = predict_nn(nn, x, y)\n",
    "            #added\n",
    "            y_temp = np.array(y_temp)\n",
    "            y_temp = np.where(y_temp==0, -1, 1)\n",
    "            y_pred = y_pred + (alph * y_temp)\n",
    "        \n",
    "        y_pred = quantizer(y_pred)\n",
    "        return y_pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for Sampling\n",
    "Takes a 1-by-N matrix and outputs result as a one-dimensional matrix\"\"\"   #approved\n",
    "\n",
    "def sampling(distr, n_samples):\n",
    "    \n",
    "    indvec = np.array(range(n_samples))\n",
    "    out_ind=[]\n",
    "    \n",
    "    for i in range (n_samples):\n",
    "        num = np.ceil(n_samples * distr[i])\n",
    "        num = int(num)\n",
    "        for j in range(num):\n",
    "            out_ind.append(indvec[i])\n",
    "    np.random.shuffle(out_ind)\n",
    "    return out_ind[0:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Function for quantizing predicted class values by custom AdaBoost implementation   \n",
    "(so that classes take either 0 or 1 values)\"\"\"             #approved\n",
    "def quantizer(y):\n",
    "    yout=[]\n",
    "    for i in range (len(y)):\n",
    "        if(y[i]<0.0):\n",
    "            yout.append(0.0)\n",
    "        else:\n",
    "            yout.append(1.0)\n",
    "    yout = np.array(yout)\n",
    "    return yout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /localhome/sahilhassan/anaconda_python/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /localhome/sahilhassan/anaconda_python/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      "1260/1260 [==============================] - 176s 140ms/step - loss: 0.7063\n",
      "Epoch 2/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6906\n",
      "Epoch 3/40\n",
      "1260/1260 [==============================] - 165s 131ms/step - loss: 0.6809\n",
      "Epoch 4/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6854\n",
      "Epoch 5/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.7131\n",
      "Epoch 6/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6979\n",
      "Epoch 7/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6714\n",
      "Epoch 8/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6498\n",
      "Epoch 9/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6437\n",
      "Epoch 10/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6560\n",
      "Epoch 11/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6373\n",
      "Epoch 12/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6569\n",
      "Epoch 13/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6395\n",
      "Epoch 14/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6319\n",
      "Epoch 15/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6034\n",
      "Epoch 16/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5974\n",
      "Epoch 17/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5968\n",
      "Epoch 18/40\n",
      "1260/1260 [==============================] - 147s 117ms/step - loss: 0.5844\n",
      "Epoch 19/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6358\n",
      "Epoch 20/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5961\n",
      "Epoch 21/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5848\n",
      "Epoch 22/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5588\n",
      "Epoch 23/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5627\n",
      "Epoch 24/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.5659\n",
      "Epoch 25/40\n",
      "1260/1260 [==============================] - 165s 131ms/step - loss: 0.5433\n",
      "Epoch 26/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6489\n",
      "Epoch 27/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5198\n",
      "Epoch 28/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5210\n",
      "Epoch 29/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6335\n",
      "Epoch 30/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5871\n",
      "Epoch 31/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5385\n",
      "Epoch 32/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.5701\n",
      "Epoch 33/40\n",
      "1260/1260 [==============================] - 163s 130ms/step - loss: 0.5028\n",
      "Epoch 34/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.4976\n",
      "Epoch 35/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5192\n",
      "Epoch 36/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5053\n",
      "Epoch 37/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5351\n",
      "Epoch 38/40\n",
      "1260/1260 [==============================] - 145s 115ms/step - loss: 0.5968\n",
      "Epoch 39/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.4615\n",
      "Epoch 40/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.4855\n",
      "Epoch 1/40\n",
      "1260/1260 [==============================] - 176s 140ms/step - loss: 0.7005\n",
      "Epoch 2/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.7042\n",
      "Epoch 3/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6901\n",
      "Epoch 4/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6843\n",
      "Epoch 5/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6851\n",
      "Epoch 6/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6962\n",
      "Epoch 7/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6785\n",
      "Epoch 8/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6771\n",
      "Epoch 9/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6684\n",
      "Epoch 10/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6702\n",
      "Epoch 11/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6618\n",
      "Epoch 12/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6691\n",
      "Epoch 13/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6700\n",
      "Epoch 14/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6497\n",
      "Epoch 15/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6490\n",
      "Epoch 16/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6645\n",
      "Epoch 17/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6571\n",
      "Epoch 18/40\n",
      "1260/1260 [==============================] - 145s 115ms/step - loss: 0.6646\n",
      "Epoch 19/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6472\n",
      "Epoch 20/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6426\n",
      "Epoch 21/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.7307\n",
      "Epoch 22/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6642\n",
      "Epoch 23/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6465\n",
      "Epoch 24/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6495\n",
      "Epoch 25/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6486\n",
      "Epoch 26/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.7562\n",
      "Epoch 27/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6787\n",
      "Epoch 28/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6653\n",
      "Epoch 29/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6220\n",
      "Epoch 30/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.7490\n",
      "Epoch 31/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6206\n",
      "Epoch 32/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6632\n",
      "Epoch 33/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6053\n",
      "Epoch 34/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.5866\n",
      "Epoch 35/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6369\n",
      "Epoch 36/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6083\n",
      "Epoch 37/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.5789\n",
      "Epoch 38/40\n",
      "1260/1260 [==============================] - 151s 120ms/step - loss: 0.5786\n",
      "Epoch 39/40\n",
      "1260/1260 [==============================] - 156s 124ms/step - loss: 0.5890\n",
      "Epoch 40/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6525\n",
      "Epoch 1/40\n",
      "1260/1260 [==============================] - 176s 140ms/step - loss: 0.7039\n",
      "Epoch 2/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6827\n",
      "Epoch 3/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6912\n",
      "Epoch 4/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6770\n",
      "Epoch 5/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6985\n",
      "Epoch 6/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.7470\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6772\n",
      "Epoch 8/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6556\n",
      "Epoch 9/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6909\n",
      "Epoch 10/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6972\n",
      "Epoch 11/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6753\n",
      "Epoch 12/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6660\n",
      "Epoch 13/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6386\n",
      "Epoch 14/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6389\n",
      "Epoch 15/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6451\n",
      "Epoch 16/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6477\n",
      "Epoch 17/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6636\n",
      "Epoch 18/40\n",
      "1260/1260 [==============================] - 151s 119ms/step - loss: 0.6312\n",
      "Epoch 19/40\n",
      "1260/1260 [==============================] - 158s 125ms/step - loss: 0.6094\n",
      "Epoch 20/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6824\n",
      "Epoch 21/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.7385\n",
      "Epoch 22/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6264\n",
      "Epoch 23/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6204\n",
      "Epoch 24/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5971\n",
      "Epoch 25/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6131\n",
      "Epoch 26/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5992\n",
      "Epoch 27/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.5759\n",
      "Epoch 28/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6135\n",
      "Epoch 29/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5970\n",
      "Epoch 30/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5910\n",
      "Epoch 31/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6365\n",
      "Epoch 32/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5577\n",
      "Epoch 33/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5555\n",
      "Epoch 34/40\n",
      "1260/1260 [==============================] - 165s 131ms/step - loss: 0.5489\n",
      "Epoch 35/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5478\n",
      "Epoch 36/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5481\n",
      "Epoch 37/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5555\n",
      "Epoch 38/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5912\n",
      "Epoch 39/40\n",
      "1260/1260 [==============================] - 142s 113ms/step - loss: 0.5915\n",
      "Epoch 40/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5697\n",
      "Epoch 1/40\n",
      "1260/1260 [==============================] - 177s 141ms/step - loss: 0.7102\n",
      "Epoch 2/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6835\n",
      "Epoch 3/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6959\n",
      "Epoch 4/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6846\n",
      "Epoch 5/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6828\n",
      "Epoch 6/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6842\n",
      "Epoch 7/40\n",
      "1260/1260 [==============================] - 163s 130ms/step - loss: 0.6722\n",
      "Epoch 8/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6671\n",
      "Epoch 9/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6668\n",
      "Epoch 10/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6565\n",
      "Epoch 11/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6810\n",
      "Epoch 12/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6648\n",
      "Epoch 13/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6468\n",
      "Epoch 14/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6526\n",
      "Epoch 15/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.6511\n",
      "Epoch 16/40\n",
      "1260/1260 [==============================] - 165s 131ms/step - loss: 0.6326\n",
      "Epoch 17/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.6266\n",
      "Epoch 18/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.7743\n",
      "Epoch 19/40\n",
      "1260/1260 [==============================] - 143s 114ms/step - loss: 0.6524\n",
      "Epoch 20/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.6042\n",
      "Epoch 21/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6313\n",
      "Epoch 22/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.6141\n",
      "Epoch 23/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.5950\n",
      "Epoch 24/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.5990\n",
      "Epoch 25/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5791\n",
      "Epoch 26/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5977\n",
      "Epoch 27/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5881\n",
      "Epoch 28/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5936\n",
      "Epoch 29/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5778\n",
      "Epoch 30/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5765\n",
      "Epoch 31/40\n",
      "1260/1260 [==============================] - 164s 130ms/step - loss: 0.6040\n",
      "Epoch 32/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5655\n",
      "Epoch 33/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.5610\n",
      "Epoch 34/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.5768\n",
      "Epoch 35/40\n",
      "1260/1260 [==============================] - 161s 128ms/step - loss: 0.5492\n",
      "Epoch 36/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5918\n",
      "Epoch 37/40\n",
      "1260/1260 [==============================] - 162s 128ms/step - loss: 0.5595\n",
      "Epoch 38/40\n",
      "1260/1260 [==============================] - 163s 129ms/step - loss: 0.5573\n",
      "Epoch 39/40\n",
      "1260/1260 [==============================] - 143s 114ms/step - loss: 0.5541\n",
      "Epoch 40/40\n",
      "1260/1260 [==============================] - 162s 129ms/step - loss: 0.5509\n",
      "Epoch 1/40\n",
      "1260/1260 [==============================] - 102s 81ms/step - loss: 0.7111\n",
      "Epoch 2/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6886\n",
      "Epoch 3/40\n",
      "1260/1260 [==============================] - 86s 69ms/step - loss: 0.6935\n",
      "Epoch 4/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6923\n",
      "Epoch 5/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6866\n",
      "Epoch 6/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6906\n",
      "Epoch 7/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6715\n",
      "Epoch 8/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6773\n",
      "Epoch 9/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6500\n",
      "Epoch 10/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.6618\n",
      "Epoch 11/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.6867\n",
      "Epoch 12/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.6624\n",
      "Epoch 13/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.6397\n",
      "Epoch 14/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6356\n",
      "Epoch 15/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.6798\n",
      "Epoch 16/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6654\n",
      "Epoch 17/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6291\n",
      "Epoch 18/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6191\n",
      "Epoch 19/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6170\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.5943\n",
      "Epoch 21/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.6151\n",
      "Epoch 22/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5987\n",
      "Epoch 23/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.7790\n",
      "Epoch 24/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.5964\n",
      "Epoch 25/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.5679\n",
      "Epoch 26/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.5648\n",
      "Epoch 27/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.6343\n",
      "Epoch 28/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5786\n",
      "Epoch 29/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5692\n",
      "Epoch 30/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.7045\n",
      "Epoch 31/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5933\n",
      "Epoch 32/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5411\n",
      "Epoch 33/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5573\n",
      "Epoch 34/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.5584\n",
      "Epoch 35/40\n",
      "1260/1260 [==============================] - 86s 68ms/step - loss: 0.5419\n",
      "Epoch 36/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5959\n",
      "Epoch 37/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5515\n",
      "Epoch 38/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.5428\n",
      "Epoch 39/40\n",
      "1260/1260 [==============================] - 87s 69ms/step - loss: 0.5266\n",
      "Epoch 40/40\n",
      "1260/1260 [==============================] - 85s 68ms/step - loss: 0.5329\n",
      "kappa value is =  0.4591401597080561\n",
      "po value is 0.7341269841269841\n",
      "agree num 925\n",
      "pe value is 0.5084252960443436\n",
      "rand_yes  0.15857772738725118\n",
      "rand_no  0.3498475686570925\n",
      "spec_yes  598\n",
      "spec_no 662\n",
      "model_yes  421\n",
      "model_no  839\n",
      "**************==============******************\n",
      "Training error is  0.26587301587301587  and kappa is  0.4591401597080561\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Main program body\"\"\"\n",
    "\"\"\"Sampling based ensamble\"\"\"\n",
    "\n",
    "###Step 1: Load Training Data\n",
    "#Done, not in a function.\n",
    "\n",
    "\n",
    "\n",
    "### Adaboost flow\n",
    "##=================================\n",
    "\n",
    "# x,y = traindata, trainlabel\n",
    "x,y = xtr, ytr\n",
    "\n",
    "# initial = D = 1/n\n",
    "nbsample = len(ytr)\n",
    "D = (1/nbsample)* np.ones((nbsample,))\n",
    "\n",
    "# variable to hold neural nets\n",
    "nn_cluster = []\n",
    "\n",
    "#variable to hold alpha values\n",
    "alpha = []\n",
    "\n",
    "for t in range (5):\n",
    "    #1. create classifier\n",
    "    shallow_nn = make_nn(10000, 8000, 7000, 6000, 5000, 4000, 2500, 1250, 650, 300, 100, 1)\n",
    "\n",
    "    #2. Train using xtr\n",
    "    train_nn(shallow_nn,x,y, 30, 50, True)\n",
    "\n",
    "    #3. Predict on xtr\n",
    "    ypred, __ = predict_nn(shallow_nn, x, y)\n",
    "\n",
    "    #4. calculate epsilon\n",
    "    epsilon = weighted_error(y,ypred,D)\n",
    "\n",
    "    #5. Calculate alpha\n",
    "    alph = 0.5*math.log10((1-epsilon)/epsilon)\n",
    "\n",
    "    #6. update D\n",
    "    D = distr_upd(D, epsilon, alph, y, ypred)\n",
    "\n",
    "    #7. append the already made classifier\n",
    "    nn_cluster.append(shallow_nn)\n",
    "\n",
    "    #8. append the alpha values captured\n",
    "    alpha.append(alph)\n",
    "\n",
    "    #9. Sample data for next iteration\n",
    "    sampled_ind = sampling(D,nbsample)\n",
    "    x , y = x[sampled_ind,:], np.array(y)[sampled_ind]\n",
    "\n",
    "\n",
    "### By this point, the classifier should be prepared, now we apply the predict function\n",
    "\n",
    "ytr_predict = ens_pred_nn(nn_cluster, xtr, ytr, alpha)\n",
    "\n",
    "### Calculating Error\n",
    "train_error = errcalc(ytr, ytr_predict)\n",
    "\n",
    "### Calculating Kappa\n",
    "\n",
    "train_kappa = kappa_calc(ytr, ytr_predict)\n",
    "\n",
    "\n",
    "## Printing Results\n",
    "\n",
    "print('**************==============******************')\n",
    "print('Training error is ', train_error, ' and kappa is ', train_kappa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Loading Testing Data</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Load Validation Data\"\"\"\n",
    "#code for reading data\n",
    "imgdir = []   #shall contain the full path of the image\n",
    "yts = []         #shall contain label of each image\n",
    "#xtr = np.matrix([[]])         #shall contain each image\n",
    "\n",
    "#xtr = np.zeros((257,262144))   #modified\n",
    "xts = np.zeros((1300,10000))\n",
    "\n",
    "#datatype = ['train', 'valid']\n",
    "#studylabel = {'negative':0, 'positive':1}\n",
    "\n",
    "BASE_DIR = 'mura_humerus'\n",
    "\n",
    "validdir = BASE_DIR + '/' + datatype[1]  #shows training directory\n",
    "\n",
    "patients = os.listdir(validdir)  #all the patient ID's are now into this\n",
    "\n",
    "i = 0\n",
    "for patient in patients:\n",
    "    directory = validdir + '/' + patient    #patient directory train/patientxxxx/\n",
    "    studies = os.listdir(directory)         #creates list of studies in each patient directory\n",
    "    for study in studies:\n",
    "        images = os.listdir(directory + '/' + study)\n",
    "        for image in images:\n",
    "            imgdir = np.append(imgdir, directory + '/' + study + '/' + image)    #add image addresses in array\n",
    "            yts = np.append(yts, studylabel[study.split('_')[1]])        #keep adding labels of each images\n",
    "            file = mpimg.imread(imgdir[i])\n",
    "            if (len(np.shape(file))>2):\n",
    "                pixels = np.matrix(file[:,:,0])     #takes matrix format of one image\n",
    "                pixels = np.reshape(pixels, (1,len(pixels)*len(pixels.T)))   #flattens the image to 1-D\n",
    "            else:\n",
    "                pixels = np.matrix(file)\n",
    "                pixels = np.reshape(pixels, (1,len(pixels)*len(pixels.T)))\n",
    "                \n",
    "            if (np.shape(pixels)[1] < 10000):    #modified from 262144\n",
    "                zeroarray = np.matrix(np.zeros((1,(10000-np.shape(pixels)[1]))))\n",
    "                pixels = np.concatenate((pixels,zeroarray), axis=1)\n",
    "                #print(pixels)\n",
    "                #xtr = np.append(xtr, np.array(file[:,:,0]), axis=0)\n",
    "                ###xtr = np.concatenate(xtr,pixels, axis=1)               #load and add images in \n",
    "                xts[i] = pixels               #load and add images in \n",
    "                #print ('i from if=', i)\n",
    "            else:\n",
    "                xts[i] = np.matrix(pixels[:,0:10000])     #modified from 262144\n",
    "                #print(pixels)\n",
    "                #print ('i from else=', i)\n",
    "                \n",
    "            i=i+1\n",
    "\n",
    "#print(i)\n",
    "xtest = xts[0:i,:]\n",
    "#print(xtest.shape)\n",
    "ytest = yts[0:i]\n",
    "#print(i)\n",
    "#print(np.shape(ytest))\n",
    "\n",
    "\n",
    "\"\"\"Normalize data\"\"\"\n",
    "for i in range (len(xtest)):\n",
    "    if np.amax(xtest[i]) != 0:\n",
    "        xtest[i] = xtest[i] / np.amax(xtest[i])\n",
    "        \n",
    "        \n",
    "\"\"\"Eliminating all zero data\"\"\"\n",
    "xts = np.zeros((np.shape(xtest)))\n",
    "yts = []\n",
    "j = 0\n",
    "i = 0\n",
    "while i < (len(ytest)):\n",
    "    if np.amax(xtest[i]) != 0:\n",
    "        xts[j] = xtest[i]\n",
    "        yts.append(ytest[i])\n",
    "        j = j+1\n",
    "    i = i+1\n",
    "    \n",
    "xts = xts[0:j,:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 10000)\n",
      "(282, 10000)\n",
      "(288,)\n",
      "(282,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Verification\"\"\"\n",
    "print(np.shape(xtest))\n",
    "print(np.shape(xts))\n",
    "print(np.shape(ytest))\n",
    "print(np.shape(yts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa value is =  0.11897203325774756\n",
      "po value is 0.5602836879432624\n",
      "agree num 158\n",
      "pe value is 0.5009053870529652\n",
      "rand_yes  0.21499170061868117\n",
      "rand_no  0.285913686434284\n",
      "spec_yes  139\n",
      "spec_no 143\n",
      "model_yes  123\n",
      "model_no  159\n",
      "**************==============******************\n",
      "Validation error is  0.4397163120567376  and kappa is  0.11897203325774756\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluating test performance\"\"\"\n",
    "\n",
    "### By this point, the classifier should be prepared, now we apply the predict function\n",
    "\n",
    "yts_predict = ens_pred_nn(nn_cluster, xts, yts, alpha)\n",
    "\n",
    "### Calculating Error\n",
    "test_error = errcalc(yts, yts_predict)\n",
    "\n",
    "### Calculating Kappa\n",
    "\n",
    "test_kappa = kappa_calc(yts, yts_predict)\n",
    "\n",
    "\n",
    "## Printing Results\n",
    "\n",
    "print('**************==============******************')\n",
    "print('Validation error is ', test_error, ' and kappa is ', test_kappa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
